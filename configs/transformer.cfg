# config for transformer AE/VAE
accelerator: 'gpu'
num_devices: 1
training: True
dataloader: 'prometheus'
checkpoint: ''
resume_training: False
project_name: 'example_transformer'
project_save_dir: '/path/to/wandb_project_save_dir/example_transformer'
model_options: 
    architecture: 'transformer' # Added for clarity, though factory might infer from filename
    in_features: 6400
    latent_dim: 64
    embed_dim: 32
    sensor_positional_encoding: False
    # VAE options (set is_vae=True to enable)
    is_vae: False
    beta_schedule: 'annealed'  # 'none', 'constant', 'annealed'
    beta_factor: 0.00001     # Relevant if beta_schedule='constant' or 'annealed'
    beta_peak_epoch: 20      # Relevant if beta_schedule='annealed'
data_options:
    train_data_files: ['/dataset/directory/1/',
                       '/dataset/directory/2/']
    train_data_file_ranges: [[0, 19], [0, 9]]
    valid_data_files: ['/dataset/directory/1/',
                       '/dataset/directory/2/']
    valid_data_file_ranges: [[20, 24], [10, 14]]
    file_chunk_size: 250000
training_options:
    batch_size: 1024
    lr: 0.0001
    lr_schedule: []
    weight_decay: 0.01
    epochs: 25
    save_epochs: 5
    num_workers: 2 