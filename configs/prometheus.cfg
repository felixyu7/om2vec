accelerator: 'gpu'
num_devices: 1
training: True
dataloader: 'prometheus'
checkpoint: ''
resume_training: False
project_name: 'om2vec'
project_save_dir: './experiment_logs'
logger: 'wandb'
model_options:
    latent_dim: 32  # Fully learned latent dimension (summary stats removed)
    embed_dim: 128
    beta_factor: 0.1
    beta_peak_epoch: 8
    eos_loss_weight: 1.0  # Weight for EOS token loss (default 1.0, configurable)
    # PyTorch Transformer Encoder HParams
    transformer_encoder_layers: 6
    transformer_encoder_heads: 8
    transformer_encoder_ff_dim: 512
    transformer_encoder_dropout: 0.1
    # PyTorch Transformer Decoder HParams
    transformer_decoder_layers: 3
    transformer_decoder_heads: 8
    transformer_decoder_ff_dim: 512
    transformer_decoder_dropout: 0.3
    
data_options:
    train_data_files: ["/n/holylfs05/LABS/arguelles_delgado_lab/Everyone/felixyu/IceCube_MC_sensors/NuMu",
                        "/n/holylfs05/LABS/arguelles_delgado_lab/Everyone/felixyu/IceCube_MC_sensors/EMinus",
                        "/n/holylfs05/LABS/arguelles_delgado_lab/Everyone/felixyu/IceCube_MC_sensors/MuMinus",
                        "/n/holylfs05/LABS/arguelles_delgado_lab/Everyone/felixyu/IceCube_MC_sensors/TauMinus",]
    valid_data_files: ["/n/holylfs05/LABS/arguelles_delgado_lab/Everyone/felixyu/IceCube_MC_sensors/NuMu"]
    train_data_file_ranges: [[0, 4], [0, 4], [0, 4], [0, 4]] # Ranges for each file in train_data_files
    valid_data_file_ranges: [[5, 7]] # Ranges for each file in valid_data_files
    shuffle_files: True
    max_seq_len_padding: 1024
    grouping_window_ns: 2.0

training_options:
    batch_size: 256
    lr: 0.0001
    lr_schedule: [10, 0.000001]
    weight_decay: 0.0001
    gradient_clip_val: 1.0
    precision: 'bf16-mixed'
    test_precision: '32-true'
    epochs: 10
    save_epochs: 1
    num_workers: 4