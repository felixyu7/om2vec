project_name: "om2vec"
project_save_dir: "./experiment_logs"
training: true
accelerator: "gpu" # or "cpu"
num_devices: 1
dataloader: "prometheus" # Corresponds to PrometheusDataModule
checkpoint: ""
resume_training: false
logger: "csv"

data_options:
  train_data_files: ["/Users/felixyu/icecube/data/prometheus/"] # Placeholder
  train_data_file_ranges: [[0, 2]]
  valid_data_files: ["/Users/felixyu/icecube/data/prometheus/"] # Placeholder
  valid_data_file_ranges: [[0, 2]]
  shuffle_files: true
  max_seq_len: 128 # Max processing length for binning/padding & decoder loop
  time_column: "t" # Column in Parquet for lists of times
  # charge_is_count: true # Implicitly, as raw_q is ones, then binned

model_name: "om2vec" # Corresponds to Om2vecModel
model_options:
  # Encoder params
  encoder_input_embed_dim_time: 64
  encoder_input_embed_dim_charge: 64
  encoder_hidden_dim: 128
  encoder_num_layers: 1
  # Latent space: total_latent_dim = 1 (for length) + learned_latent_dim
  latent_dim: 64 # Total z dim. If 64, then 1 for length, 63 learned.
  # Decoder (NTPP) params
  decoder_hidden_dim: 128
  decoder_num_layers: 1
  ntpp_time_dist: "LogNormal"
  ntpp_charge_dist: "Normal"
  # KL annealing parameters
  kl_beta_start: 0.001 # Initial beta value for KL divergence
  kl_beta_end: 1.0     # Final beta value for KL divergence
  kl_anneal_epochs: 50 # Number of epochs to reach kl_beta_end from kl_beta_start

training_options:
  batch_size: 32
  lr: 0.001
  lr_schedule: [100, 0.000001]
  weight_decay: 0.01
  epochs: 100
  save_epochs: 5
  num_workers: 4
  precision: "32-true"
  test_precision: "32-true"