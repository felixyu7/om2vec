accelerator: 'gpu'
num_devices: 1
training: True
dataloader: 'prometheus'
checkpoint: ''
resume_training: False
project_name: 'example_testing'
project_save_dir: './experiment_logs'
logger: 'csv'
model_options: 
    latent_dim: 64
    embed_dim: 32 # Used for Transformer d_model
    beta_factor: 0.00001
    beta_peak_epoch: 20
    sensor_positional_encoding: False # Handling TBD in VAE forward pass

    # PyTorch Transformer Encoder HParams
    transformer_encoder_layers: 6
    transformer_encoder_heads: 8
    transformer_encoder_ff_dim: 128 # Example: embed_dim * 4
    transformer_encoder_dropout: 0.1

    # Zuko NSF (Decoder) HParams
    flow_transforms: 5
    flow_bins: 8
    flow_hidden_dim: 128 # For hidden_features list e.g. [128, 128]
    flow_hidden_layers: 2

data_options:
    train_data_files: ['/Users/felixyu/icecube/data/prometheus_om2vec/']
    # IMPORTANT: Ranges are now 0-indexed Python slice parameters. 
    # E.g., [0, 19] means files from index 0 up to (but not including) index 19.
    # Adjust these values based on your dataset structure.
    train_data_file_ranges: [[0, 2]] 
    valid_data_files: ['/Users/felixyu/icecube/data/prometheus_om2vec/']
    valid_data_file_ranges: [[0, 2]]
    shuffle_files: True 
    max_seq_len_padding: 512 # Defines sequence length for padding
    grouping_window_ns: 2.0
    time_column: "hits_t"

training_options:
    batch_size: 32
    lr: 0.0001
    lr_schedule: [] # Example: [10, 20] for steps at epoch 10 and 20
    weight_decay: 0.01
    epochs: 25
    save_epochs: 5
    num_workers: 2
